{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c9feda9-5557-4fbe-a9e1-4f8ac42b99c3",
   "metadata": {},
   "source": [
    "# LangChain Usecases \n",
    "\n",
    "https://github.com/gkamradt/langchain-tutorials/blob/main/LangChain%20Cookbook%20Part%202%20-%20Use%20Cases.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ecc0bf5b-01a9-4104-bf82-422ce08a0cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0.330\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "\n",
    "from langchain.embeddings import GooglePalmEmbeddings\n",
    "from langchain.llms import GooglePalm\n",
    "import google.generativeai\n",
    "import os\n",
    "\n",
    "llm = GooglePalm(\n",
    "    google_api_key=os.getenv('GOOGLE_API_KEY'),\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "# prompts = ['Explain the difference between effective and affective with examples']\n",
    "# llm_result = llm._generate(prompts)\n",
    "\n",
    "# print(llm_result.generations[0][0].text)\n",
    "print(langchain.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eeee1aa7-65b1-48e7-b585-a3432ea7d2e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mGooglePalm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcache\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseCallbackHandler\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseCallbackManager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcallback_manager\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseCallbackManager\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtags\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmetadata\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mclient\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mgoogle_api_key\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmodel_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'models/text-bison-001'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtemperature\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtop_p\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtop_k\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmax_output_tokens\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m      Google PaLM models.\n",
       "\u001b[0;31mInit docstring:\u001b[0m\n",
       "Create a new model by parsing and validating input data from keyword arguments.\n",
       "\n",
       "Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
       "\u001b[0;31mFile:\u001b[0m           ~/.local/share/virtualenvs/python-bard-d03NoAlC/lib/python3.11/site-packages/langchain/llms/google_palm.py\n",
       "\u001b[0;31mType:\u001b[0m           ModelMetaclass\n",
       "\u001b[0;31mSubclasses:\u001b[0m     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "GooglePalm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0351157c-2042-4752-b55f-083348f53773",
   "metadata": {},
   "source": [
    "## Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9339614-7aa3-418f-b15d-b52a6ad80667",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "# Create our template.\n",
    "template = \"\"\"\n",
    "%INSTRUCTIONS:\n",
    "Please summarize the following piece of text.\n",
    "Response in a manner that a 5 year old would understand.\n",
    "\n",
    "%TEXT:\n",
    "{text}\n",
    "\"\"\"\n",
    "\n",
    "# Create a LangChain prompt template that we can insert values to later\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=['text'],\n",
    "    template=template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0542c7fb-1d82-4b7b-8e71-dc0539c2915b",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusing_text = \"\"\"\n",
    "For the next 130 years, debate raged.\n",
    "Some scientists called Prototaxites a lichen, others a fungus, and still others clung to the notion that it was some kind of tree.\n",
    "“The problem is that when you look up close at the anatomy, it’s evocative of a lot of different things, but it’s diagnostic of nothing,” says Boyce, an associate professor in geophysical sciences and the Committee on Evolutionary Biology.\n",
    "“And it’s so damn big that when whenever someone says it’s something, everyone else’s hackles get up: ‘How could you have a lichen 20 feet tall?’”\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f76b972f-c626-417f-ba1c-3f0f475caf85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Prompt Begin -------\n",
      "\n",
      "%INSTRUCTIONS:\n",
      "Please summarize the following piece of text.\n",
      "Response in a manner that a 5 year old would understand.\n",
      "\n",
      "%TEXT:\n",
      "\n",
      "For the next 130 years, debate raged.\n",
      "Some scientists called Prototaxites a lichen, others a fungus, and still others clung to the notion that it was some kind of tree.\n",
      "“The problem is that when you look up close at the anatomy, it’s evocative of a lot of different things, but it’s diagnostic of nothing,” says Boyce, an associate professor in geophysical sciences and the Committee on Evolutionary Biology.\n",
      "“And it’s so damn big that when whenever someone says it’s something, everyone else’s hackles get up: ‘How could you have a lichen 20 feet tall?’”\n",
      "\n",
      "\n",
      "------- Prompt End -------\n"
     ]
    }
   ],
   "source": [
    "print (\"------- Prompt Begin -------\")\n",
    "\n",
    "final_prompt = prompt.format(text=confusing_text)\n",
    "print(final_prompt)\n",
    "\n",
    "print (\"------- Prompt End -------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "07253f1f-867a-4544-9dee-59cdfb67d67e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scientists argued for 130 years about what Prototaxites is. Some said it is a lichen, some said it is a fungus, and some said it is a tree. The problem is that it looks like a lot of different things, but it is not clear what it is.\n"
     ]
    }
   ],
   "source": [
    "output = llm(final_prompt)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b82e364-4303-46f6-9878-d3d425c8bbd3",
   "metadata": {},
   "source": [
    "## Summaries of Longer Text\n",
    "\n",
    "We will use `wikipedia` [^1] to get articles online and then summarize it with LLM.\n",
    "\n",
    "[^1]: https://python.langchain.com/docs/integrations/tools/wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "204796e9-f246-4744-bbaa-50525112fd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qqq wikipedia transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a207e398-0f3c-47eb-a419-7ebcdd5d3cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import WikipediaQueryRun\n",
    "from langchain.utilities import WikipediaAPIWrapper\n",
    "\n",
    "wikipedia = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())\n",
    "text = wikipedia.run('idempotency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "325dd625-0cb4-46fc-88ef-268cc8c66045",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2057"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1f500c28-de4a-4435-9d8e-c1cfb785d2a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page: Idempotence\n",
      "Summary: Idempotence (UK: , US: ) is the property of certain operations in mathematics and computer science whereby they can be applied multiple times without changing the result beyond the initial application. The concept of idempotence arises in a number of places in abstract alg\n"
     ]
    }
   ],
   "source": [
    "print(text[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "427d6a94-0761-4bb7-aef2-7a3104b12e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 704 tokens in your text\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import GooglePalm\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "llm = GooglePalm(\n",
    "    google_api_key=os.getenv('GOOGLE_API_KEY'),\n",
    "    temperature=0.0\n",
    ")\n",
    "num_tokens = llm.get_num_tokens(text)\n",
    "print(f'There are {num_tokens} tokens in your text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f0f128b5-bd6d-4a20-aca8-bf2b537809a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You now have 3 docs intead of 1 piece of text\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(separators=[\"\\n\\n\", \"\\n\"], \n",
    "                                               chunk_size=1000, \n",
    "                                               chunk_overlap=350)\n",
    "docs = text_splitter.create_documents([text])\n",
    "\n",
    "print (f\"You now have {len(docs)} docs intead of 1 piece of text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "38c2e925-9c44-4053-9948-55c583a97b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verbose=True optional to see what is getting sent to the LLM\n",
    "chain = load_summarize_chain(llm=llm, \n",
    "                             chain_type='map_reduce',\n",
    "                             verbose=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "64dfa6c9-332e-4f4b-a432-fb9439a7c681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MapReduceDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"Page: Idempotence\n",
      "Summary: Idempotence (UK: , US: ) is the property of certain operations in mathematics and computer science whereby they can be applied multiple times without changing the result beyond the initial application. The concept of idempotence arises in a number of places in abstract algebra (in particular, in the theory of projectors and closure operators) and functional programming (in which it is connected to the property of referential transparency).\n",
      "The term was introduced by American mathematician Benjamin Peirce in 1870 in the context of elements of algebras that remain invariant when raised to a positive integer power, and literally means \"(the quality of having) the same power\", from idem + potence (same + power).\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"Page: Idempotency of entailment\n",
      "Summary: Idempotency of entailment is a property of logical systems that states that one may derive the same consequences from many instances of a hypothesis as from just one.   This property can be captured by a structural rule called contraction, and in such systems one may say that entailment is idempotent if and only if contraction is an admissible rule.\n",
      "Rule of contraction: from\n",
      "\n",
      "A,C,C → Bis derived\n",
      "\n",
      "A,C → B.Or in sequent calculus notation,\n",
      "\n",
      "  \n",
      "    \n",
      "      \n",
      "        \n",
      "          \n",
      "            \n",
      "              Γ\n",
      "              ,\n",
      "              C\n",
      "              ,\n",
      "              C\n",
      "              ⊢\n",
      "              B\n",
      "            \n",
      "            \n",
      "              Γ\n",
      "              ,\n",
      "              C\n",
      "              ⊢\n",
      "              B\n",
      "            \n",
      "          \n",
      "        \n",
      "      \n",
      "    \n",
      "    {\\displaystyle {\\frac {\\Gamma ,C,C\\vdash B}{\\Gamma ,C\\vdash B}}}\n",
      "  In linear and affine logic, entailment is not idempotent.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"Page: Stock issues\n",
      "Summary: In the formal speech competition genre known as policy debate, a widely accepted doctrine or \"debate theory\" divides the argument elements of supporting the resolution affirmative into five subtopical issues, called the stock issues. Stock issues are sometime referred to as on-case arguments or simply on-case or case arguments as opposed off-case arguments.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"Idempotence is the property of an operation that produces the same result when applied multiple times.\n",
      "\n",
      "Idempotency of entailment states that one may derive the same consequences from many instances of a hypothesis as from just one.\n",
      "\n",
      "Stock issues are five subtopical issues that divide the argument elements of supporting the resolution affirmative in policy debate.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Idempotence: an operation that produces the same result when applied multiple times.\n",
      "Idempotency of entailment: deriving the same consequences from many instances of a hypothesis as from just one.\n",
      "Stock issues: five subtopical issues that divide the argument elements of supporting the resolution affirmative in policy debate.\n"
     ]
    }
   ],
   "source": [
    "output = chain.run(docs)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd86099f-c7fb-4918-82ce-f1a4950271a7",
   "metadata": {},
   "source": [
    "# Question & Answering Using Documents As Context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb542341-740f-4210-99e2-a9f5a2e835a0",
   "metadata": {},
   "source": [
    "## Using Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "63c0f069-0871-4bf2-b05c-4a454e5ddaa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!pip install -qqq faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "61d4bc91-05b6-4e92-815e-30c65943fe34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import GooglePalm\n",
    "\n",
    "# The vectorstore we'll be using\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# The LangChain component we'll use to get the documents\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# The easy document loader for text\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "# The embedding engine that will convert our text to vectors\n",
    "from langchain.embeddings import GooglePalmEmbeddings\n",
    "\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "llm = GooglePalm(\n",
    "    google_api_key=google_api_key,\n",
    "    temperature=0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d05dec0c-b74a-4537-9143-0ce15e6a287e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 1 document\n",
      "You have 2057 characters in that document\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema.document import Document\n",
    "\n",
    "# Instead of using document loader, we load it directly from string.\n",
    "doc = [Document(page_content=text)]\n",
    "print (f\"You have {len(doc)} document\")\n",
    "print (f\"You have {len(doc[0].page_content)} characters in that document\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "23d07c92-d8f2-4773-b74d-8a8ac1c8a109",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=400)\n",
    "docs = text_splitter.split_documents(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8c83c5e7-9a11-4bd8-ba1f-0f79e8ae1fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now you have 3 documents that have an average of 684 characters (smaller pieces)\n"
     ]
    }
   ],
   "source": [
    "# Get the total number of characters so we can see the average later\n",
    "num_total_characters = sum([len(x.page_content) for x in docs])\n",
    "\n",
    "print (f\"Now you have {len(docs)} documents that have an average of {num_total_characters / len(docs):,.0f} characters (smaller pieces)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9976a7f9-0eda-4f2d-9e04-c12973924243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get your embeddings engine ready\n",
    "embeddings = GooglePalmEmbeddings(google_api_key=google_api_key)\n",
    "\n",
    "# Embed your documents and combine with the raw text in a pseudo db. Note: This will make an API call to OpenAI\n",
    "docsearch = FAISS.from_documents(docs, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feeb06f9-d7ff-4cb2-9825-b4c3e515f541",
   "metadata": {},
   "source": [
    "# Create your retrieval engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "247c214c-5f27-4ae4-ab43-5015c2fe85db",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=docsearch.as_retriever())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "97c50453-2109-4534-948f-cf832cc3a1a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a property of logical systems'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is the article about?\"\n",
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a84c01c0-863e-4eda-91b9-3ad2021ae70b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the same consequences from many instances of a hypothesis as from just one'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is the advantage of idempotency?\"\n",
    "qa.run(query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
